{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as kb\n",
    "import ml_util_4_head\n",
    "import time\n",
    "from scipy import signal\n",
    "import importlib\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanAbsoluteError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials to train with:  [ 1  2  3  4  5  6  7  8  9 10 11] subjects to train with:  [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "unilateral_4_head\n"
     ]
    }
   ],
   "source": [
    "#Experiment_options\n",
    "importlib.reload(ml_util_4_head)\n",
    "window_size = 200\n",
    "num_snapshots_in_sequence = 300\n",
    "use_smooth = False\n",
    "do_filter = False\n",
    "predict_future_by = 0 # num samples to predict into the future\n",
    "use_side_flag = False\n",
    "subjects_to_train_with = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "\n",
    "sides = ['LEFT', 'RIGHT']\n",
    "\n",
    "trial_nums = 1+np.arange(11)\n",
    "print('trials to train with: ', trial_nums, 'subjects to train with: ', subjects_to_train_with)\n",
    "\n",
    "root_folder = \"unilateral_4_head\"\n",
    "print(root_folder)\n",
    "sequence_len = num_snapshots_in_sequence + window_size - 1\n",
    "training_instances = np.empty(shape=[0,sequence_len, 12], dtype=np.float32)\n",
    "files_to_train_with = ml_util_4_head.get_files_to_use(root_folder, subjects_to_train_with, sides, trial_nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_actual, y_pred):\n",
    "    mask = kb.greater_equal(y_actual, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    custom_loss = tf.math.reduce_sum(\n",
    "        kb.square(mask*(y_actual-y_pred)))/tf.math.reduce_sum(mask)\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_for_ramp(y_actual, y_pred):\n",
    "    # Create a mask where `y_actual` is not equal to -100\n",
    "    mask = kb.not_equal(y_actual, -100)\n",
    "    # Cast the mask to float32\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # Use the mask to ignore the values where `y_actual` is -100\n",
    "    masked_squared_error = kb.square(mask * (y_actual - y_pred))\n",
    "    # Calculate the sum of the squared errors where the mask is True\n",
    "    numerator = tf.math.reduce_sum(masked_squared_error)\n",
    "    # Calculate the sum of the mask values (essentially the count of non-masked elements)\n",
    "    denominator = tf.math.reduce_sum(mask)\n",
    "    # To avoid division by zero, add a small constant to the denominator\n",
    "    denominator = tf.where(tf.equal(denominator, 0), tf.constant(1, dtype=tf.float32), denominator)\n",
    "    # Compute the mean squared error while ignoring the masked values\n",
    "    custom_loss_value = numerator / denominator\n",
    "    return custom_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_ss_col(ss_col):\n",
    "    flag = 0  # Flags: 0 -> looking for first positive, 1 -> searching for 1, 2 -> holding 1, 3 -> setting to -1, 4 -> holding 0\n",
    "    counter = 0\n",
    "    first_positive_found = False  # Indicator for the first positive value\n",
    "\n",
    "    for i in range(len(ss_col)):\n",
    "        if not first_positive_found:\n",
    "            if ss_col[i] > 0:\n",
    "                first_positive_found = True\n",
    "                flag = 1  # Start looking for the next 1\n",
    "            else:\n",
    "                ss_col[i] = -1  # Set to -1 until the first positive value is found\n",
    "        else:\n",
    "            if flag == 1 and ss_col[i] == 1:\n",
    "                # When 1 is found, start holding at 1\n",
    "                flag = 2\n",
    "                counter = 1\n",
    "            elif flag == 2:\n",
    "                # Hold 1 for 15 points\n",
    "                if counter < 15:\n",
    "                    ss_col[i] = 1\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    # Then go to -1 for the next 10 points\n",
    "                    flag = 3\n",
    "                    counter = 1\n",
    "                    ss_col[i] = -1\n",
    "            elif flag == 3:\n",
    "                # Hold -1 for 10 points\n",
    "                if counter < 10:\n",
    "                    ss_col[i] = -1\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    # Then go back to 0 and start looking for 1 again\n",
    "                    flag = 4\n",
    "                    counter = 0\n",
    "            elif flag == 4 and ss_col[i] != 0:\n",
    "                # As soon as the value starts increasing, start looking for 1 again\n",
    "                flag = 1\n",
    "    \n",
    "    return ss_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_2023v2(window_size,\n",
    "                                      filter_sizes,\n",
    "                                      kernel_sizes,\n",
    "                                      dilations,\n",
    "                                      num_channels=8,\n",
    "                                      batch_norm_insertion_pts=[2],\n",
    "                                      sp_dense_sizes=[20, 10],\n",
    "                                      ss_dense_sizes=[20, 10],\n",
    "                                      v_dense_sizes=[20, 10],\n",
    "                                      r_dense_sizes=[20, 10],\n",
    "                                      do_fix_input_dim=False):\n",
    "    if len(filter_sizes) != len(kernel_sizes)+1:\n",
    "        raise ValueError(\n",
    "            'Must provide one more filter size than kernel size--last kernel size is calculated')\n",
    "    current_output_size = window_size  # Track for final conv layer\n",
    "\n",
    "    #Use None in dim 0 to allow variable input length.\n",
    "    #Use window_size to fix size--helpful for debugging dimensions\n",
    "    if do_fix_input_dim:\n",
    "        input_layer = tf.keras.layers.Input(\n",
    "            shape=(window_size, num_channels), name='my_input_layer')\n",
    "    else:\n",
    "        input_layer = tf.keras.layers.Input(\n",
    "            shape=(None, num_channels), name='my_input_layer')\n",
    "\n",
    "    # outputs = []\n",
    "    z = input_layer\n",
    "    # Conv1D for each head\n",
    "    for layer_idx in range(len(kernel_sizes)):\n",
    "        z = tf.keras.layers.Conv1D(filters=filter_sizes[layer_idx], kernel_size=kernel_sizes[layer_idx],\n",
    "                                    dilation_rate=dilations[layer_idx], activation='relu')(z)\n",
    "        if layer_idx in batch_norm_insertion_pts:\n",
    "            z = tf.keras.layers.BatchNormalization()(z)\n",
    "        current_output_size = current_output_size - \\\n",
    "            dilations[layer_idx]*kernel_sizes[layer_idx] + dilations[layer_idx]\n",
    "    if current_output_size < 1:\n",
    "        raise ValueError('layers shrink the cnn too much')\n",
    "    else:\n",
    "        print('adding final conv layer of kernel size: ', current_output_size)\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            filters=filter_sizes[-1], kernel_size=current_output_size, activation='relu')(z)\n",
    "    for num_neurons in sp_dense_sizes:\n",
    "        z = tf.keras.layers.Dense(num_neurons, activation='relu')(z)\n",
    "    output_stance_phase = tf.keras.layers.Dense(1, name='stance_phase_output')(z)    \n",
    "\n",
    "\n",
    "    current_output_size = window_size      \n",
    "    z = input_layer\n",
    "    # Conv1D for each head\n",
    "    for layer_idx in range(len(kernel_sizes)):\n",
    "        z = tf.keras.layers.Conv1D(filters=filter_sizes[layer_idx], kernel_size=kernel_sizes[layer_idx],\n",
    "                                    dilation_rate=dilations[layer_idx], activation='relu')(z)\n",
    "        if layer_idx in batch_norm_insertion_pts:\n",
    "            z = tf.keras.layers.BatchNormalization()(z)\n",
    "        current_output_size = current_output_size - \\\n",
    "            dilations[layer_idx]*kernel_sizes[layer_idx] + dilations[layer_idx]\n",
    "    if current_output_size < 1:\n",
    "        raise ValueError('layers shrink the cnn too much')\n",
    "    else:\n",
    "        print('adding final conv layer of kernel size: ', current_output_size)\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            filters=filter_sizes[-1], kernel_size=current_output_size, activation='relu')(z)\n",
    "    for num_neurons in ss_dense_sizes:\n",
    "        z = tf.keras.layers.Dense(num_neurons, activation='relu')(z)\n",
    "    output_stance_swing = tf.keras.layers.Dense(\n",
    "        1, activation='sigmoid', name='stance_swing_output')(z)\n",
    "    \n",
    "    current_output_size = window_size\n",
    "    z = input_layer\n",
    "    # Conv1D for each head\n",
    "    for layer_idx in range(len(kernel_sizes)):\n",
    "        z = tf.keras.layers.Conv1D(filters=filter_sizes[layer_idx], kernel_size=kernel_sizes[layer_idx],\n",
    "                                    dilation_rate=dilations[layer_idx], activation='relu')(z)\n",
    "        if layer_idx in batch_norm_insertion_pts:\n",
    "            z = tf.keras.layers.BatchNormalization()(z)\n",
    "        current_output_size = current_output_size - \\\n",
    "            dilations[layer_idx]*kernel_sizes[layer_idx] + dilations[layer_idx]\n",
    "    if current_output_size < 1:\n",
    "        raise ValueError('layers shrink the cnn too much')\n",
    "    else:\n",
    "        print('adding final conv layer of kernel size: ', current_output_size)\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            filters=filter_sizes[-1], kernel_size=current_output_size, activation='relu')(z)\n",
    "    for num_neurons in v_dense_sizes:\n",
    "        z = tf.keras.layers.Dense(num_neurons, activation='relu')(z)\n",
    "    velocity = tf.keras.layers.Dense(1, name='velocity_output')(z) \n",
    "\n",
    "    z = input_layer\n",
    "    current_output_size = window_size\n",
    "    # Conv1D for each head\n",
    "    for layer_idx in range(len(kernel_sizes)):\n",
    "        z = tf.keras.layers.Conv1D(filters=filter_sizes[layer_idx], kernel_size=kernel_sizes[layer_idx],\n",
    "                                    dilation_rate=dilations[layer_idx], activation='relu')(z)\n",
    "        if layer_idx in batch_norm_insertion_pts:\n",
    "            z = tf.keras.layers.BatchNormalization()(z)\n",
    "        current_output_size = current_output_size - \\\n",
    "            dilations[layer_idx]*kernel_sizes[layer_idx] + dilations[layer_idx]\n",
    "    if current_output_size < 1:\n",
    "        raise ValueError('layers shrink the cnn too much')\n",
    "    else:\n",
    "        print('adding final conv layer of kernel size: ', current_output_size)\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            filters=filter_sizes[-1], kernel_size=current_output_size, activation='relu')(z)\n",
    "    for num_neurons in r_dense_sizes:\n",
    "        z = tf.keras.layers.Dense(num_neurons, activation='relu')(z)\n",
    "    ramp = tf.keras.layers.Dense(1, name='ramp_output')(z)  \n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_layer], outputs=[\n",
    "    output_stance_phase, output_stance_swing, velocity, ramp])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:  (6402, 499, 8) shape of y_gp_train:  (6402, 300) shape of y_ss_train:  (6402, 300) \n",
      "shape of x_valid:  (1600, 499, 8) shape of y_gp_valid:  (1600, 300) shape of y_ss_valid:  (1600, 300)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "for myfile in files_to_train_with:\n",
    "    data = ml_util_4_head.load_file(myfile)\n",
    "    \n",
    "    ss_col = data[:,-4]\n",
    "    ss_col = manipulate_ss_col(ss_col)\n",
    "    data[:,-4] = ss_col\n",
    "    # MAX \n",
    "    ramp_col = data[:,-2]\n",
    "    ramp_col[ss_col==0]=-100\n",
    "    data[:,-2] = ramp_col\n",
    "    num_rows, num_cols = data.shape\n",
    "    num_rows_to_drop = num_rows % sequence_len\n",
    "    data = data[0:-num_rows_to_drop]\n",
    "    new_num_rows, num_cols = data.shape\n",
    "    num_sequences = new_num_rows/sequence_len\n",
    "    new_data_shape = (int(num_sequences), sequence_len, num_cols)\n",
    "    new_instances = data.reshape(new_data_shape)\n",
    "    training_instances = np.append(training_instances, new_instances, axis=0)\n",
    "\n",
    "shuffled_training_instances = tf.random.shuffle(training_instances) \n",
    "num_channels = 8\n",
    "x = shuffled_training_instances[:, :, :num_channels]\n",
    "y_v = shuffled_training_instances[:, window_size-1:,-1]\n",
    "y_r = shuffled_training_instances[:, window_size-1:,-2]\n",
    "y_sp = shuffled_training_instances[:, window_size-1:,-4]\n",
    "y_ss = shuffled_training_instances[:, window_size-1:,-3]\n",
    "\n",
    "\n",
    "split_fraction = 0.8\n",
    "split_num = int(np.rint(split_fraction*x.shape[0]))\n",
    "x_train = x[:split_num,:,:]\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "y_sp_train = y_sp[:split_num,:]\n",
    "y_ss_train = y_ss[:split_num,:]\n",
    "y_r_train = y_r[:split_num,:]\n",
    "y_v_train = y_v[:split_num,:]\n",
    "\n",
    "x_valid = x[split_num:,:,:]\n",
    "y_sp_valid = y_sp[split_num:,:]\n",
    "y_ss_valid = y_ss[split_num:,:]\n",
    "y_r_valid = y_r[split_num:,:]\n",
    "y_v_valid = y_v[split_num:,:]\n",
    "\n",
    "\n",
    "print('shape of x_train: ', x_train.shape, 'shape of y_gp_train: ', y_sp_train.shape, 'shape of y_ss_train: ', y_ss_train.shape,\n",
    "'\\nshape of x_valid: ', x_valid.shape, 'shape of y_gp_valid: ', y_sp_valid.shape, 'shape of y_ss_valid: ', y_ss_valid.shape)\n",
    "print(x_train.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(y_v_valid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(y_sp_valid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(y_ss_valid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(y_r_valid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding final conv layer of kernel size:  74\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " my_input_layer (InputLayer  [(None, None, 8)]            0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, None, 44)             2860      ['my_input_layer[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, None, 44)             176       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, None, 44)             15532     ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, None, 44)             176       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, None, 44)             15532     ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, None, 44)             143308    ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, None, 44)             143308    ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, None, 44)             143308    ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, None, 44)             143308    ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 23)             1035      ['conv1d_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, None, 23)             1035      ['conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, None, 23)             1035      ['conv1d_5[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, None, 23)             1035      ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 23)             552       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, None, 23)             552       ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, None, 23)             552       ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, None, 23)             552       ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, None, 23)             552       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, None, 23)             552       ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, None, 23)             552       ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, None, 23)             552       ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, None, 23)             552       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, None, 23)             552       ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, None, 23)             552       ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, None, 23)             552       ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " stance_phase_output (Dense  (None, None, 1)              24        ['dense_3[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stance_swing_output (Dense  (None, None, 1)              24        ['dense_7[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " velocity_output (Dense)     (None, None, 1)              24        ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " ramp_output (Dense)         (None, None, 1)              24        ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 618368 (2.36 MB)\n",
      "Trainable params: 618192 (2.36 MB)\n",
      "Non-trainable params: 176 (704.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # BUILD MODEL\n",
    "importlib.reload(ml_util_4_head)\n",
    "num_channels = 8\n",
    "do_fix_input_dim=False\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = construct_model_2023v2(window_size=window_size,\n",
    "    filter_sizes=[44,44,44,44], kernel_sizes=[8,8,8], dilations=[6,6,6], num_channels=8,\n",
    "    batch_norm_insertion_pts=[0,1], sp_dense_sizes=[23,23,23,23], ss_dense_sizes=[23,23,23,23], v_dense_sizes=[23,23,23,23], r_dense_sizes=[23,23,23,23], do_fix_input_dim=do_fix_input_dim)\n",
    "\n",
    "print(model.summary())\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=[ml_util_4_head.custom_loss, 'binary_crossentropy', ml_util_4_head.custom_loss, custom_loss_for_ramp], loss_weights=[4,1,0.3,0.1], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"test.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint( filename, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model.fit(x=x_train, y=[y_sp_train, y_ss_train, y_v_train, y_r_train], \n",
    "    batch_size=32, epochs=50, validation_data=(x_valid,[y_sp_valid, y_ss_valid, y_v_valid, y_r_valid]), \n",
    "    callbacks=[es, mc], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('letssee.h5', custom_objects={'custom_loss': custom_loss, 'custom_loss_for_ramp' : custom_loss_for_ramp})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials to test with:  [1, 2, 5, 6] subjects to train with:  [10]\n",
      "unilateral_4_head\n"
     ]
    }
   ],
   "source": [
    "subjects_to_test_with = [10]\n",
    "trial_nums = [1,2,5,6]\n",
    "print('trials to test with: ', trial_nums, 'subjects to train with: ', subjects_to_test_with)\n",
    "sides = ['RIGHT', 'LEFT']\n",
    "\n",
    "root_folder = \"unilateral_4_head\"\n",
    "print(root_folder)\n",
    "\n",
    "files_to_test_with = ml_util_4_head.get_files_to_use(root_folder, subjects_to_test_with, sides, trial_nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "rmses = []\n",
    "rmses_vel = []\n",
    "rmses_isp = []\n",
    "rmses_ramp = []\n",
    "\n",
    "\n",
    "\n",
    "num_channels = 8\n",
    "\n",
    "for myfile in files_to_test_with:\n",
    "    data = ml_util_4_head.load_file(myfile)\n",
    "\n",
    "    ss_col_test = data[window_size-1:,-4]\n",
    "\n",
    "\n",
    "    # MAX \n",
    "    ramp_col_test = data[window_size-1:,-2]\n",
    "    ramp_col_test[ss_col_test==0]=-100\n",
    "    data[window_size-1:,-2] = ramp_col_test\n",
    "\n",
    "\n",
    "\n",
    "    x_test = tf.expand_dims(data[:,:num_channels], axis=0)\n",
    "    x_test=tf.cast(x_test, dtype = tf.float32)\n",
    "    y_sp_test = data[window_size-1:,-4]\n",
    "    y_ss_test = data[window_size-1:,-3]\n",
    "    y_v_test = data[window_size-1:,-1]\n",
    "    y_r_test = data[window_size-1:,-2]\n",
    "\n",
    "\n",
    "    model_outputs = model.predict(x=x_test)    \n",
    "    y_sp_predict, y_ss_predict, y_v_predict, y_r_predict = tf.squeeze(model_outputs)\n",
    "\n",
    "\n",
    "    valid_indices = (y_sp_test[7000:-900] != -1) & (y_ss_test[7000:-900] == True)\n",
    "    err = y_sp_predict[7000:-900][valid_indices] - y_sp_test[7000:-900][valid_indices]\n",
    "\n",
    "\n",
    "    rmse = np.sqrt(np.mean(np.square(err))) \n",
    "    rmses.append(rmse)\n",
    "\n",
    "\n",
    "    err_isp = y_ss_predict[7000:-900]-y_ss_test[7000:-900]\n",
    "\n",
    "    # Mask nan values\n",
    "    err_isp = err_isp[~np.isnan(err_isp)]\n",
    "    rmse_isp = np.sqrt(np.mean(np.square(err_isp))) \n",
    "    rmses_isp.append(rmse_isp)\n",
    "\n",
    "\n",
    "\n",
    "    err_vel = y_v_predict - y_v_test\n",
    "    err_vel = err_vel[y_v_test!=-1].numpy()\n",
    "    rmse_vel = np.sqrt(np.mean(np.square(err_vel)))\n",
    "    rmses_vel.append(rmse_vel)\n",
    "\n",
    "\n",
    "    # Mask where y_r_test is not -100\n",
    "    mask = y_r_test != -100\n",
    "\n",
    "    # Apply mask to calculate RMSE\n",
    "    err_ramp = y_r_predict[mask] - y_r_test[mask]\n",
    "    rmse_ramp = np.sqrt(np.mean(np.square(err_ramp)))\n",
    "    rmses_ramp.append(rmse_ramp)\n",
    "\n",
    "\n",
    "mean_rmses_sp = np.mean(rmses)\n",
    "mean_rmses_isp = np.mean(rmses_isp)\n",
    "mean_rmses_vel = np.mean(rmses_vel)\n",
    "mean_rmses_ramp = np.mean(rmses_ramp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030393934"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmses_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11119409"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmses_isp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08286287"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmses_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0033383"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmses_ramp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
